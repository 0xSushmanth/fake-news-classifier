{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faux Filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:56.503004600Z",
     "start_time": "2023-05-15T08:21:13.933292300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaive_bayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultinomialNB\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# source: tensorflow/core/framework/function.proto\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (unknown location)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'builder' from 'google.protobuf.internal' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tf\u001b[38;5;241m.\u001b[39m__version__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/__init__.py:37\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/python/eager/context.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mabsl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewriter_config_pb2\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow/core/framework/function_pb2.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Generated by the protocol buffer compiler.  DO NOT EDIT!\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# source: tensorflow/core/framework/function.proto\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"Generated protocol buffer code.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m builder \u001b[38;5;28;01mas\u001b[39;00m _builder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor \u001b[38;5;28;01mas\u001b[39;00m _descriptor\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m descriptor_pool \u001b[38;5;28;01mas\u001b[39;00m _descriptor_pool\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'builder' from 'google.protobuf.internal' (unknown location)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.809661500Z",
     "start_time": "2023-05-15T08:21:56.503004600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data from csv\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.833025500Z",
     "start_time": "2023-05-15T08:21:57.809661500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>Specter of Trump Loosens Tongues, if Not Purse...</td>\n",
       "      <td>David Streitfeld</td>\n",
       "      <td>PALO ALTO, Calif.  —   After years of scorning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian warships ready to strike terrorists ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>#NoDAPL: Native American Leaders Vow to Stay A...</td>\n",
       "      <td>Common Dreams</td>\n",
       "      <td>Videos #NoDAPL: Native American Leaders Vow to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>Tim Tebow Will Attempt Another Comeback, This ...</td>\n",
       "      <td>Daniel Victor</td>\n",
       "      <td>If at first you don’t succeed, try a different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>Keiser Report: Meme Wars (E995)</td>\n",
       "      <td>Truth Broadcast Network</td>\n",
       "      <td>42 mins ago 1 Views 0 Comments 0 Likes 'For th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              title  \\\n",
       "0  20800  Specter of Trump Loosens Tongues, if Not Purse...   \n",
       "1  20801  Russian warships ready to strike terrorists ne...   \n",
       "2  20802  #NoDAPL: Native American Leaders Vow to Stay A...   \n",
       "3  20803  Tim Tebow Will Attempt Another Comeback, This ...   \n",
       "4  20804                    Keiser Report: Meme Wars (E995)   \n",
       "\n",
       "                    author                                               text  \n",
       "0         David Streitfeld  PALO ALTO, Calif.  —   After years of scorning...  \n",
       "1                      NaN  Russian warships ready to strike terrorists ne...  \n",
       "2            Common Dreams  Videos #NoDAPL: Native American Leaders Vow to...  \n",
       "3            Daniel Victor  If at first you don’t succeed, try a different...  \n",
       "4  Truth Broadcast Network  42 mins ago 1 Views 0 Comments 0 Likes 'For th...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.896039300Z",
     "start_time": "2023-05-15T08:21:57.825023700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 20800 number of rows and 5 number of columns for training.\n",
      "There are 5200 number of rows and 4 number of columns for testing.\n"
     ]
    }
   ],
   "source": [
    "# Displaying rows and columns in dataset\n",
    "print(\"There are {} number of rows and {} number of columns for training.\".format(train.shape[0],train.shape[1]))\n",
    "print(\"There are {} number of rows and {} number of columns for testing.\".format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.905041100Z",
     "start_time": "2023-05-15T08:21:57.839026400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the null values in training data.\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.965055Z",
     "start_time": "2023-05-15T08:21:57.856030800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "title     122\n",
       "author    503\n",
       "text        7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the null values in testing data.\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling nan values in dataset using empty spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.974056800Z",
     "start_time": "2023-05-15T08:21:57.873034700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def handle_nan(train_data,test_data):\n",
    "    '''Input: Data to the function containing Nan values.\n",
    "       Output : Cleaned data containing no Nan values.\n",
    "       Function: Cleaning Nan values.\n",
    "     '''\n",
    "    train = train_data.fillna(\" \")\n",
    "    test  = test_data.fillna(\" \")\n",
    "    return train,test\n",
    "\n",
    "train,test = handle_nan(train,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.974056800Z",
     "start_time": "2023-05-15T08:21:57.904040600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Creating a variable \"merged\" by merging columns \"title\" and \"author\"\n",
    "train[\"merged\"] = train[\"title\"]+\" \"+train[\"author\"]\n",
    "test[\"merged\"]  = test[\"title\"]+\" \"+test[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.975057400Z",
     "start_time": "2023-05-15T08:21:57.920046Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Seperating Independent and dependent features\n",
    "X = train.drop(columns=['label'],axis=1)\n",
    "y = train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:21:57.975057400Z",
     "start_time": "2023-05-15T08:21:57.938048700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Creating One-Hot Representations\n",
    "messages = X.copy()\n",
    "messages.reset_index(inplace=True)\n",
    "messages_test = test.copy()\n",
    "messages_test.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "**1. Firstly, we will process and use stopwords.**\n",
    "**2. Next, to avoid false predictions or ambiguity with upper and lowercase, we will convert them to lowercase.**\n",
    "**3. Next, all the sentences are tokenized into words.**\n",
    "**4. We will use stemming to the tokenized words for quick preprocessing.**\n",
    "**5. Next, words are joined together and stored in the corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:10.463779Z",
     "start_time": "2023-05-15T08:21:57.951051700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flynn hillari clinton big woman campu breitbart daniel j flynn'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing data preprocessing on column 'title'\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def perform_preprocess(data):\n",
    "    '''Input: Data to be processed\n",
    "       Output: Preprocessed data\n",
    "    '''\n",
    "    corpus = []\n",
    "    for i in range(0,len(data)):\n",
    "        review = re.sub('[^a-zA-Z]',' ',data['merged'][i])\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [ps.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    return corpus\n",
    "    \n",
    "train_corpus = perform_preprocess(messages)\n",
    "test_corpus  = perform_preprocess(messages_test)\n",
    "train_corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:10.523792600Z",
     "start_time": "2023-05-15T08:23:10.463779Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'russian warship readi strike terrorist near aleppo'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below code converts the pre-processed words to one-hot vectors in the range of vocabulary size=5000. This is done to obtain numerical feature matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:10.855868300Z",
     "start_time": "2023-05-15T08:23:10.480783Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "one_hot_train = [one_hot(word,vocab_size) for word in train_corpus]\n",
    "one_hot_test  = [one_hot(word,vocab_size) for word in test_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:10.871873600Z",
     "start_time": "2023-05-15T08:23:10.856868300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4833, 2158, 3975, 1108, 4946, 4554, 1507]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below code creates an embedding layer which applies \"pre\" padding to the one-hot encoded features with sentence length = 20. Padding is applied so that the length of every sequence in the dataset should be same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:10.952894800Z",
     "start_time": "2023-05-15T08:23:10.872873300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 4847 4139 3332]\n",
      " [   0    0    0 ... 3333 3264 2745]\n",
      " [   0    0    0 ...  919  409 1073]\n",
      " ...\n",
      " [   0    0    0 ... 3168 2657 4896]\n",
      " [   0    0    0 ... 1168 1851 4146]\n",
      " [   0    0    0 ... 4150  951 3777]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Embedding Representation\n",
    "sent_length = 20\n",
    "embedd_docs_train = pad_sequences(one_hot_train,padding='pre',maxlen=sent_length)\n",
    "embedd_docs_test  = pad_sequences(one_hot_test,padding='pre',maxlen=sent_length)\n",
    "print(embedd_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:10.995904400Z",
     "start_time": "2023-05-15T08:23:10.951894300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 2987  951 2197]\n",
      " [   0    0    0 ... 4946 4554 1507]\n",
      " [   0    0    0 ... 1568   76 1816]\n",
      " ...\n",
      " [   0    0    0 ... 2987 3043 3175]\n",
      " [   0    0    0 ... 4833 1289 4296]\n",
      " [   0    0    0 ... 2987 2914  518]]\n"
     ]
    }
   ],
   "source": [
    "print(embedd_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:11.006906500Z",
     "start_time": "2023-05-15T08:23:10.967897500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Converting Embedding repr. to array\n",
    "x_final = np.array(embedd_docs_train)\n",
    "y_final = np.array(y)\n",
    "x_test_final = np.array(embedd_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:11.007907200Z",
     "start_time": "2023-05-15T08:23:10.982901300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20800, 20), (20800,), (5200, 20))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions of prev. array repr.\n",
    "x_final.shape,y_final.shape,x_test_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Split into Test and Train.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:11.051916600Z",
     "start_time": "2023-05-15T08:23:10.998904900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.1, random_state=42, stratify = y_final)\n",
    "X_train, x_valid, Y_train, y_valid = train_test_split(x_train, y_train, test_size=0.1, random_state=42, stratify = y_train)\n",
    "x_test_final = x_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:11.491017600Z",
     "start_time": "2023-05-15T08:23:11.045915700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create CountVectorizer object\n",
    "cv = CountVectorizer(max_features=5000)\n",
    "\n",
    "# fit_transform on train data\n",
    "x_train_cv = cv.fit_transform(train_corpus).toarray()\n",
    "\n",
    "# transform on test data\n",
    "x_test_cv = cv.transform(test_corpus).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Models\n",
    "**In this phase, several models are created and evaluated against various metrics shown using classification report.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Logistic Regresssion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression: A linear model that uses a logistic function to model the probability of a binary response variable. It is commonly used for binary classification problems, but can also be extended to multi-class classification problems.**\n",
    "\n",
    "**The logistic regression model predicts the probability of a binary response variable (y) given a set of input features (x) using the logistic function:\n",
    "p(y=1|x) = 1 / (1 + exp(-z))\n",
    "where z = b0 + b1x1 + b2x2 + ... + bn*xn is the linear combination of the input features and their corresponding weights (b0, b1, b2, ..., bn).\n",
    "The weights are estimated from the training data using maximum likelihood estimation or other optimization techniques.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:11.637049200Z",
     "start_time": "2023-05-15T08:23:11.490015700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72      1039\n",
      "           1       0.72      0.70      0.71      1041\n",
      "\n",
      "    accuracy                           0.72      2080\n",
      "   macro avg       0.72      0.72      0.72      2080\n",
      "weighted avg       0.72      0.72      0.72      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Logistic Regression model with a maximum of 900 iterations\n",
    "model_1 = LogisticRegression(max_iter=900)\n",
    "\n",
    "# Train the model on the training data\n",
    "model_1.fit(X_train, Y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "pred_1 = model_1.predict(x_test)\n",
    "\n",
    "# Generate a classification report to evaluate the performance of the model\n",
    "cr1 = classification_report(y_test, pred_1)\n",
    "\n",
    "# Print the classification report to the console\n",
    "print(cr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Naive Bayes**\n",
    "**A probabilistic model that uses Bayes' theorem to predict the probability of a class given a set of features. It is commonly used for text classification problems, where the features are the frequencies of words in a document.**\n",
    "\n",
    "**The Naive Bayes model predicts the probability of a binary response variable (y) given a set of input features (x) using Bayes' theorem:\n",
    "p(y=1|x) = p(x|y=1) * p(y=1) / p(x)\n",
    "where p(x|y=1) is the likelihood of the input features given the positive class, p(y=1) is the prior probability of the positive class, and p(x) is the marginal probability of the input features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:11.700063200Z",
     "start_time": "2023-05-15T08:23:11.635048Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.60      0.65      1039\n",
      "           1       0.65      0.75      0.70      1041\n",
      "\n",
      "    accuracy                           0.68      2080\n",
      "   macro avg       0.68      0.67      0.67      2080\n",
      "weighted avg       0.68      0.68      0.67      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Multinomial Naive Bayes model\n",
    "model_2 = MultinomialNB()\n",
    "\n",
    "# Train the model on the training data\n",
    "model_2.fit(X_train, Y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "pred_2 = model_2.predict(x_test)\n",
    "\n",
    "# Generate a classification report to evaluate the performance of the model\n",
    "cr2 = classification_report(y_test, pred_2)\n",
    "\n",
    "# Print the classification report to the console\n",
    "print(cr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Decision Trees**\n",
    "**A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. It recursively splits the data into subsets based on the values of the features, and assigns a class label to each leaf node.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:11.840094400Z",
     "start_time": "2023-05-15T08:23:11.666055300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93      1039\n",
      "           1       0.92      0.93      0.93      1041\n",
      "\n",
      "    accuracy                           0.93      2080\n",
      "   macro avg       0.93      0.93      0.93      2080\n",
      "weighted avg       0.93      0.93      0.93      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Decision Tree Classifier model\n",
    "model_3 = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model_3.fit(X_train, Y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "pred_3 = model_3.predict(x_test)\n",
    "\n",
    "# Generate a classification report to evaluate the performance of the model\n",
    "cr3 = classification_report(y_test, pred_3)\n",
    "\n",
    "# Print the classification report to the console\n",
    "print(cr3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Random Forest**\n",
    "**An ensemble model(An ensemble model is a machine learning model that combines multiple individual models to improve the accuracy and robustness of the predictions.) that combines multiple decision trees to improve the accuracy and robustness of the predictions. It randomly selects a subset of features and data points for each tree, and aggregates the predictions of all the trees to make the final prediction.An ensemble model that combines multiple decision trees to improve the accuracy and robustness of the predictions. It randomly selects a subset of features and data points for each tree, and aggregates the predictions of all the trees to make the final prediction.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The prediction of a random forest model can be represented by the following formula:\n",
    "y = mode(y1, y2, ..., yn)\n",
    "where y1, y2, ..., yn are the predictions of the individual trees, and mode() is the function that returns the most common prediction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:14.616857800Z",
     "start_time": "2023-05-15T08:23:11.843122300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.87      0.91      1039\n",
      "           1       0.88      0.95      0.91      1041\n",
      "\n",
      "    accuracy                           0.91      2080\n",
      "   macro avg       0.91      0.91      0.91      2080\n",
      "weighted avg       0.91      0.91      0.91      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier model\n",
    "model_4 = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model_4.fit(X_train, Y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "pred_4 = model_4.predict(x_test)\n",
    "\n",
    "# Generate a classification report to evaluate the performance of the model\n",
    "cr4 = classification_report(y_test, pred_4)\n",
    "\n",
    "# Print the classification report to the console\n",
    "print(cr4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. XGBOOST**\n",
    "**An optimized implementation of gradient boosting(Gradient Boosting is a machine learning technique that combines multiple weak models to create a strong predictive model) that uses a combination of tree-based models and linear models to improve the accuracy and speed of the predictions. It uses a gradient descent algorithm to iteratively improve the predictions by minimizing a loss function.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The prediction of an XGBoost model can be represented by the following formula:\n",
    "y = sum(wi * yi)\n",
    "where wi is the weight of the i-th tree, and yi is the prediction of the i-th tree. The weights and predictions are determined by the gradient descent algorithm, which minimizes a loss function that measures the difference between the predicted and actual values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:23:15.020947900Z",
     "start_time": "2023-05-15T08:23:14.616857800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1039\n",
      "           1       0.98      0.99      0.98      1041\n",
      "\n",
      "    accuracy                           0.98      2080\n",
      "   macro avg       0.98      0.98      0.98      2080\n",
      "weighted avg       0.98      0.98      0.98      2080\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an XGBoost Classifier model\n",
    "model_5 = XGBClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model_5.fit(X_train, Y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "pred_5 = model_5.predict(x_test)\n",
    "\n",
    "# Generate a classification report to evaluate the performance of the model\n",
    "cr5 = classification_report(y_test, pred_5)\n",
    "\n",
    "# Print the classification report to the console\n",
    "print(cr5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabulating the results of various implemented models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:26:56.350307200Z",
     "start_time": "2023-05-15T08:26:56.335303300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_5,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
